{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 - Regression and Classification\n",
    "\n",
    "In previous weeks we have looked at the steps needed in preparing different types of data for use by machine learning algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "# Description at http://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
    "\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the different models in scikit-learn follow a consistent structure. \n",
    "\n",
    "* The class is passed any parameters needed at initialization. In this case none are needed.\n",
    "* The fit method takes the features and the target as the parameters X and y.\n",
    "* The predict method takes an array of features and returns the predicted values\n",
    "\n",
    "These are the basic components with additional methods added when needed. For example, classifiers also have \n",
    "\n",
    "* A predict_proba method that gives the probability that a sample belongs to each of the classes.\n",
    "* A predict_log_proba method that gives the log of the probability that a sample belongs to each of the classes.\n",
    "\n",
    "## Evaluating models\n",
    "\n",
    "Before we consider whether we have a good model, or which model to choose, we must first decide on how we will evaluate our models.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "As part of our evaluation having a single number with which to compare models can be very useful. Choosing a metric that is as close a representation of our goal as possible enables many models to be automatically compared. This can be important when choosing model parameters or comparing different types of algorithm. \n",
    "\n",
    "Even if we have a metric we feel is reasonable it can be worthwhile considering in detail the predictions made by any model. Some questions to ask:\n",
    "\n",
    "* Is the model sufficiently sensitive for our use case?\n",
    "* Is the model sufficiently specific for our use case?\n",
    "* Is there any systemic bias?\n",
    "* Does the model perform equally well over the distribution of features?\n",
    "* How does the model perform outside the range of the training data?\n",
    "* Is the model overly dependent on one or two samples in the training dataset?\n",
    "\n",
    "The metric we decide to use will depend on the type of problem we have (regression or classification) and what aspects of the prediction are most important to us. For example, a decision we might have to make is between:\n",
    "\n",
    "* A model with intermediate errors for all samples\n",
    "* A model with low errors for the majority of samples but with a small number of samples that have large errors.\n",
    "\n",
    "For these two situations in a regression task we might choose mean_squared_error and mean_absolute_error.\n",
    "\n",
    "There are lists for [regression metrics](http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) and [classification metrics](http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics).\n",
    "\n",
    "We can apply the mean_squared_error metric to the linear regression model on the diabetes dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limiting our model analysis to a single number, although initially seemingly unimpressive, gives us a variety of options. As one example, we can perform a permutation test to determine whether we might see this performance by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, validation, and test datasets\n",
    "\n",
    "When evaluating different models the approach taken above is not going to work. Particularly for models with high variance, that overfit the training data, we will get very good performance on the training data but perform no better than chance on new data.\n",
    "\n",
    "For example, DecisionTreeRegressor and KNeighborsRegressor if poorly implemented will simply learn a one-to-one mapping of the data it is trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how our model truly performs we need to evaluate the performance on previously unseen samples. The general approach is to divide a dataset into training, validation and test datasets. Each model is trained on the training dataset. Multiple models can then be compared by evaluating the model against the validation dataset. There is still the potential of choosing a model that performs well on the validation dataset by chance so a final check is made against a test dataset.\n",
    "\n",
    "This unfortunately means that part of our, often expensively gathered, data can't be used to train our model. Although it is important to leave out a test dataset an alternative approach can be used for the validation dataset. Rather than just building one model we can build multiple models, each time leaving out a different validation dataset. Our validation score is then the average across each of the models. This is known as cross-validation.\n",
    "\n",
    "Scikit-learn provides classes to support cross-validation but a simple solution can also be implemented directly. Below we will separate out a test dataset to evaluate the nearest neighbor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model types\n",
    "\n",
    "Scikit-learn includes a variety of [different models](http://scikit-learn.org/stable/supervised_learning.html). The most commonly used algorithms probably include the following:\n",
    "\n",
    "* Regression\n",
    "* Support Vector Machines\n",
    "* Nearest neighbors\n",
    "* Decision trees\n",
    "* Ensembles & boosting\n",
    "\n",
    "### Regression\n",
    "\n",
    "We have already seen several examples of regression. The basic form is: \n",
    "\n",
    "$$f(X) =  \\beta_{0}  +  \\sum_{j=1}^p X_j\\beta_j$$\n",
    "\n",
    "Each feature is multipled by a coefficient and then the sum returned. This value is then transformed for classification to limit the value to the range 0 to 1.\n",
    "\n",
    "\n",
    "### Support Vector Machines\n",
    "\n",
    "Support vector machines attempt to project samples into a higher dimensional space such that they can be divided by a hyperplane. A good explanation can be found in [this article](http://noble.gs.washington.edu/papers/noble_what.html).\n",
    "\n",
    "### Nearest neighbors\n",
    "\n",
    "Nearest neighbor methods identify a number of samples from the training set that are close to the new sample and then return the average or most common value depending on the task. \n",
    "\n",
    "### Decision trees\n",
    "\n",
    "Decision trees attempt to predict the value of a new sample by learning simple rules from the training samples.\n",
    "\n",
    "### Ensembles & boosting\n",
    "\n",
    "Ensembles are combinations of other models. Combining different models together can improve performance by boosting generalizability. An average or most common value from the models is returned.\n",
    "\n",
    "Boosting builds one model and then attempts to reduce the errors with the next model. At each stage the bias in the model is reduced. In this way many weak predictors can be combined into one much more powerful predictor.\n",
    "\n",
    "I often begin with an ensemble or boosting approach as they typically give very good performance without needing to be carefully optimized. Many of the other algorithms are sensitive to their parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter selection\n",
    "\n",
    "Many of the models require several different parameters to be specified. Their performance is typically heavily influenced by these parameters and choosing the best values is vital in developing the best model.\n",
    "\n",
    "Some models have alternative implementations that handle parameter selection in an efficient way.\n",
    "\n",
    "An example is [LassoCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an expanded example in [the documentation](http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html#example-linear-model-plot-lasso-model-selection-py).\n",
    "\n",
    "There are also general classes to handle parameter selection for situations when dedicated classes are not available. As we will often have parameters in preprocessing steps these [general classes](http://scikit-learn.org/stable/modules/grid_search.html) will be used much more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Load the handwritten digits dataset and choose an appropriate metric\n",
    "2. Divide the data into a training and test dataset\n",
    "4. Build a RandomForestClassifier on the training dataset, using cross-validation to evaluate performance\n",
    "5. Choose another classification algorithm and apply it to the digits dataset. \n",
    "6. Use grid search to find the optimal parameters for the chosen algorithm.\n",
    "7. Comparing the true values with the predictions from the best model identify the numbers that are most commonly confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
